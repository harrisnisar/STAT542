{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b99af5c",
   "metadata": {},
   "source": [
    "# Basic calculus\n",
    "\n",
    "1. Calculate the derivative of $f(x)$<br>\n",
    "    (a) $f(x) = e^x$ <br>\n",
    "    $$f'(x) = e^x$$\n",
    "    (b) $f(x) = \\log(1 + x)$<br>\n",
    "    $$f'(x) = \\frac{1}{1+x}$$\n",
    "    c) $f(x) = \\log(1 + e^x)$<br>\n",
    "    $$f'(x) = \\frac{e^x}{1+x}$$\n",
    "\n",
    "2. Taylor expansion. Let $f$: $\\mathbb{R} \\rightarrow \\mathbb{R}$ be a twice differentiable function. Please write down the first three terms of its Taylor expansion at point $x = 1$.<br>\n",
    "$$\n",
    "P(x) = f(a)+f'(a)(x-1)+\\frac{f''(x)}{2}(x-1)^2\n",
    "$$\n",
    "Notes: A Taylor series is a series that is used to create an estimate (guess) of what a function looks like. There is also a special kind of Taylor series called a Maclaurin series.<br>\n",
    "The theory behind the Taylor series is that if a point is chosen on the coordinate plane (x- and y-axes), then it is possible to guess what a function will look like in the area around that point. This is done by taking the derivatives of the function and adding them all together. The idea is that it is possible to add the infinite number of derivatives and come up with a single finite sum. A Taylor series shows a function as the sum of an infinite series. The sum's terms are taken from the function's derivatives. A Taylor series for a function $f$ at a point $a$ looks like:\n",
    "$$\n",
    "f(x-a) = \\sum_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^n\n",
    "$$\n",
    "Here, we are getting the exact solution at that point because we are performing an infinite sum. This cannot be done on the computer, where instead we approxiamte that by trucating the summation at a particular n. \n",
    "\n",
    "3. For the infinite sum $\\sum_{n=1}^\\infty \\frac{1}{n^\\alpha}$, where $\\alpha$ is a positive real number, give the exact range of $\\alpha$ such that the series converges. \n",
    "$$\\alpha>1$$\n",
    "Notes: this is a p-series which is a special case of a harmonic series. The harmonic series looks like this:\n",
    "$$\\sum_{n=0}^{\\infty}\\frac{1}{n}$$\n",
    "The harmonic (harmonic because of overtones) series is just a summation of infinite positive terms so it diverges. The p-series is a more general form of the harmonic series. If p is greater than 1 it will converge because we are adding more or less adding zero's after a certain point which is the point to which the series will converge to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5e731",
   "metadata": {},
   "source": [
    "# Linear algebra\n",
    "\n",
    "1. What is the eigen-decomposition of a real symmetric matrix $A_{n \\times n}$? Write down one form of that decomposition and explain each term in your formula. Based on these terms, suppose all eigenvalues are positive, derive $A^{-1/2}$.\n",
    "\n",
    "    Eigen-decomposition can only be done on square matricies that are diaganolizable. Square matricies have the same rows and colums. If $A_{n \\times n}$ is not symmetric, but has n linearly independent eigen vectors, we say the matrix is diagnolizable. A diaganolizable matrix is a matrix that can be represented as diagnol matrix. A diagnol matrix has non-zero entries only along its diagnol:\n",
    "    $$\n",
    "        S^{-1}AS=D\n",
    "    $$\n",
    "    where $S\\in R^{n*n}$ that has the eigenvectors of A and $D\\in R{n*n}$ and is a diaganol matrix that contains the eigenvalues of A along its diaganol. \n",
    "    Eigen-decomposition just rearranges this equation to yield the following:\n",
    "    $$\n",
    "    A=SDS^{-1}\n",
    "    $$\n",
    "    In the case of A being a symmetric, we know that it will have n real and n orthogonal eigenvectors, meaning it is diaganolizable. Furthermore, since it has orthognal eigen vectors, $S^{-1}=S^{T}$. Therefore, the eigen-decomposition becomes:\n",
    "    $$\n",
    "    A=SDS^{T}\n",
    "    $$\n",
    "    This technique can be handy to calculate the power of a matrix. In general,\n",
    "    $$\n",
    "    A^n=SD^{n}S^{-1}\n",
    "    $$\n",
    "    Therefore,\n",
    "    $$\n",
    "    A^{-1/2}=SD^{-1/2}S^{T}\n",
    "    $$\n",
    "\n",
    "2. What is a symmetric positive definite matrix $A_{n \\times n}$? Give one of the equivalent definitions and explain your notation.\n",
    "\n",
    "    Because the matrix is symmetric, we can say that $A=A^T$. Symmetrix matricies are seen a lot in machine learning (covariance, hessians). In order to understand what it means to be positive definite, we look at the quadratic form of the matrix. Explicitly, given the matrix $A$ and a vector $x\\in{R^n}$, the scalar quantatity as a result of the following operation: \n",
    "    $$\n",
    "    xA^Tx\n",
    "    $$\n",
    "    is the quadratic form of the matrix. This quantatiy is a calculating a dot product between the input x and the output Ax. Now if a matrix has a quadratic form $>0, \\forall{x}$, the matrix can be called positive definite. If this value is $\\geq{0}, \\forall{x}$, the matrix is positive semi-definite. If this value is $\\leq{0}, \\forall{x}$, the matrix is negative semi-definite. If a matrix has a quadratic form $<0, \\forall{x}$, the matrix can be called negative definite. The defiteness of a matrix helps us in determining the convexity of an operation or of a loss function. A positive definite matrix is convex, meaning we can optimize it with iterative algorithms (gradient descent).\n",
    "\n",
    "3. True/False. If you claim a statement is false, explain why. For two real matrices $A_{m \\times n}$ and $B_{n \\times m}$ <br>\n",
    "    (a) Rank$(A)$ = $\\min\\{m, n\\}$ \n",
    "    \n",
    "    False, Rank$(A)\\leq\\min\\{m, n\\}$\n",
    "    \n",
    "    (b) If $m = n$, then trace$(A)$ = $\\sum_{i=1}^n A_{ii}$ \n",
    "    \n",
    "    True\n",
    "    \n",
    "    (c) If $A$ is a symmetric matrix, then all eigenvalues of $A$ are real \n",
    "    \n",
    "    True\n",
    "    \n",
    "    (d) If $A$ is a symmetric matrix, $\\lambda_1$ and $\\lambda_2$ are two distinct eigen-values and $v_1$,$v_2$ are the corresponding eigen-vectors, then it is possible that $v_1^T v_2 > 0$. \n",
    "    \n",
    "    False, the eigen-vectors of a symmetric matrix are orthogonal, meaning $v_1^T v_2 = 0$\n",
    "    \n",
    "    (e) If $A$ is a symmetric matrix,  $v_1$,$v_2$ are two distinct eigen-vectors of $\\lambda$, then it is possible that $v_1^T v_2 > 0$. \n",
    "    \n",
    "    False, the even if eigen-values are not distinct, eigen-vectors of a symmetric matrix are orthogonal, meaning $v_1^T v_2 = 0$\n",
    "    \n",
    "    (f) trace(ABAB) = trace(AABB) \n",
    "    \n",
    "    False, you cannot even compute the RHS because AA has mismatched dimensions ($n\\neq{m}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e81336",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "1. $X_1$, $X_2$, $\\ldots$, $X_n$ are i.i.d. ${\\cal N}(\\mu, \\sigma^2)$ random variables, where $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$ is finite. Let $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$. \n",
    "\n",
    "    (a) What is an unbiased estimator? Is $\\bar{X}_n$ an unbiased estimator of $\\mu$?\n",
    "    \n",
    "    The bias of an estimator is how different the expected value of the estimator is from the true value of the parameter being estimated. More concretely, say you are estimating a parameter, $\\theta$ with an esitmator $\\hat{\\theta}$. Then the bias of the estimator is calculated as follow:\n",
    "    $$\n",
    "    Bias(\\hat{\\theta}, \\theta) = E[\\hat{\\theta}]-\\theta\n",
    "    $$\n",
    "    An estimator is said to be unbiased if the bias equals 0 or,\n",
    "    $$\n",
    "    E[\\hat{\\theta}]=\\theta\n",
    "    $$\n",
    "    Intuitively, we might want this to be a property of our estimator because it means in the long run (expectation) it is doing a good job estimating our paramater. \n",
    "    As far as the case where we are using $\\bar{X}_n$ (sample mean) as an estimator of $\\mu$ (population mean), the sample mean is an unbiased estimator of the population mean because \n",
    "    $$\n",
    "    E(\\bar{x})=E(\\frac{1}{n} \\sum_{i=1}^n X_i)=\\frac{1}{n}\\sum_{i=1}^n E(X_i)=\\frac{1}{n}n\\mu=\\mu\n",
    "    $$\n",
    "    \n",
    "    (b) What is $E[(\\bar{X}_n)^2]$ in terms of $n, \\mu, \\sigma$?\n",
    "    $$\n",
    "    var(\\bar{x})=E[(\\bar{X}_n)^2]-E[\\bar{X}_n]^2\\\\\n",
    "    E[(\\bar{X}_n)^2]=var(\\bar{x})+E[\\bar{X}_n]^2\\\\\n",
    "    E[(\\bar{X}_n)^2]=\\frac{\\sigma^2}{n}+\\mu^2\n",
    "    $$\n",
    "    \n",
    "    (c) Give an unbiased estimator of $\\sigma^2$.\n",
    "    $$\n",
    "    S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2\n",
    "    $$\n",
    "    This is almost the intuitive definition of the sample variance. The only thing a little weird is the $\\frac{1}{n-1}$ normalization constant. This can be understood as a correction factor to ensure the estimator is a good/intuitive estimator, as well as unbiased. \n",
    "    \n",
    "    (d) What is a consistent estimator? Is $\\bar{X}_n$ a consistent estimator of $\\mu$?\n",
    "    \n",
    "    An estimator is called consistent when as the number of data points (n) used to generate the estimator increases indefinitely, the estimator approaches the true value of the paramter. More concretely, say you are estimating a parameter, $\\theta$ with an esitmator $\\hat{\\theta}$. Then the estimator is a consistent one if $Var(\\hat{\\mu})\\rightarrow0$ as $n\\rightarrow\\infty$. The sample mean calculated this way is consistent because it is itself normally distributed with a mean $\\mu$ and a varaince $\\frac{\\sigma^2}{n}$. The variance here clearly approaches inifinity as n does, therefore it is consistent.\n",
    "\n",
    "2. Suppose $X_{p \\times 1}$ is a vector of covariates, $\\beta_{p \\times 1}$ is a vector of unknown parameters, $\\epsilon$ is the unobserved random noise and we assume the linear model relationship $y = X^T \\beta + \\epsilon$. Suppose we have $n$ i.i.d. samples from this linear model, and the observed data can be written using the matrix form: $\\mathbf{y}_{n \\times 1} = \\mathbf{X}_{n\\times p} \\beta_{p \\times 1} + \\boldsymbol \\epsilon_{n \\times 1}$.\n",
    "    \n",
    "    (a) If we want to estimate the unknown $\\beta$ using a least square method, what is the objective/loss function $L(\\beta)$ to obtain $\\widehat \\beta$?\n",
    "    \n",
    "    The loss for the $i^{th}$ example is \n",
    "    $$L(\\beta) = (\\mathbf{X}^{(i)}-\\mathbf{y}^{(i)})^2$$\n",
    "    where  $\\mathbf{X}^{(i)}$ is the $i^{th}$ row of $\\mathbf{X}$ and $\\mathbf{y}^{(i)}$ is the $i^{th}$ row of $\\mathbf{y}$. This is the L-2 loss common in regression problems.\n",
    "    \n",
    "    (b) What is the solution of $\\widehat \\beta$? Represent the solution using the observed data $\\mathbf{y}$ and $\\mathbf{X}_{n\\times p}$. Note that you may assume that $\\mathbf{X}^T \\mathbf{X}$ is invertible.\n",
    "    \n",
    "    To solve, we take our cost function, $J(\\beta)=(\\mathbf{X}\\beta-\\mathbf{y})^T(\\mathbf{X}\\beta-\\mathbf{y})$ (the cost is just the loss averaged over all examples), and find its gradient with respect to $\\beta$. Remember that our cost is a function that takes in a matrix and returns a scalar ( $J(\\theta): R^n \\rightarrow R$ ):\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\beta) &= \\nabla_\\beta\\frac{1}{2}(\\mathbf{X}\\beta-\\mathbf{y})^T(X\\beta-\\mathbf{y})\\\\\n",
    "                        &= \\frac{1}{2}\\nabla_\\theta((\\mathbf{X}\\beta)^T\\mathbf{X}\\beta-(\\mathbf{X}\\beta)^T\\mathbf{y}\n",
    "                           -\\mathbf{y}^T(\\mathbf{X}\\beta)+\\mathbf{y}^T\\mathbf{y})\\\\\n",
    "                        &= \\frac{1}{2}\\nabla_\\beta(\\beta^T(\\mathbf{X}^T\\mathbf{X})\\beta -\\mathbf{y}^T(\\mathbf{X}\\beta) \n",
    "                           -\\mathbf{y}^T(\\mathbf{X}\\beta))\\\\\n",
    "                        &= \\frac{1}{2}\\nabla_\\beta(\\beta^T(\\mathbf{X}^T\\mathbf{X})\\beta -2(\\mathbf{X}^T\\mathbf{y})^T\\beta)\\\\\n",
    "                        &= \\frac{1}{2}(2\\mathbf{X}^T\\mathbf{X}\\beta-2\\mathbf{X}^T\\mathbf{y})\\\\\n",
    "                        &= \\mathbf{X}^T\\mathbf{X}\\beta-\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Setting this equal to 0 gives us the normal equations:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T\\mathbf{X}\\beta=\\mathbf{X}^T\\mathbf{y}\n",
    "$$\n",
    "\n",
    "Our optimal beta ($\\widehat \\beta$):\n",
    "\n",
    "$$\n",
    "\\widehat \\beta=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ccba63",
   "metadata": {},
   "source": [
    "# Programming\n",
    "\n",
    "1. Use the following code to generate a set of $n$ observations $\\mathbf{y}_{n \\times 1}$ and $\\mathbf{X}_{n\\times p}$. Follow the previously established formula to solve for the least square estimator $\\widehat \\beta$. Note that you must write your own code, instead of using existing functions such as `lm()`. In addition, what should you do if you are asked to add an intercept term $\\beta_0$ into your estimation (even the true $\\beta_0 = 0$ in our data generator)? \n",
    "```{r}\n",
    "set.seed(1)\n",
    "n = 100; p = 5\n",
    "X = matrix(rnorm(n * p), n, p)\n",
    "y = X %*% c(1, 0, 0, 1, -1) + rnorm(n)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7dcda56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta (true): [ 1.  0.  0.  1. -1.]\n",
      "beta_hat (estimate): [ 1.01740332  0.09935171 -0.03501228  0.88735656 -0.74824799]\n",
      "Average squared difference between beta (true) and beta_hat (estimate): 0.01749342291415676\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# reproduced the R code provided to generate the data in Python\n",
    "def generate(n, p, beta):\n",
    "    np.random.seed(0) \n",
    "    X = np.random.normal(size=(n,p))\n",
    "    np.random.seed(0) \n",
    "    noise = np.random.normal(size=n)\n",
    "    y = X @ beta + noise\n",
    "    return y, X, beta\n",
    "\n",
    "def solve(X, y):\n",
    "    return inv(X.T@X)@X.T@y\n",
    "\n",
    "n = 100\n",
    "p = 5\n",
    "beta = np.array([1.0,0.0,0.0,1.0,-1.0])\n",
    "\n",
    "(y,X,beta) = generate(n, p, beta)\n",
    "beta_hat = solve(X, y)\n",
    "print(f'beta (true): {beta}')\n",
    "print(f'beta_hat (estimate): {beta_hat}')\n",
    "print(f'Average squared difference between beta (true) and beta_hat (estimate): {np.mean(np.power(beta-beta_hat,2))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a1d70",
   "metadata": {},
   "source": [
    "In order to add an intercept term, we append a 1 to each row of $\\mathbf{X}$. This makes $X\\in R^{n\\times{(p+1)}}$. Now the same code gives us a beta vector containing 6 paramters, with the last one being our estimate of $\\beta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722a32a",
   "metadata": {},
   "source": [
    "2. Perform a simulation study to check the consistency of a sample mean estimator $\\bar{X}_n$. Please save your random seed so that the results can be replicated by others. <br>\n",
    "    (a) Generate a set of $n = 20$ i.i.d. observations from uniform(0, 1) distribution and calculate the sample mean $\\bar{X}_n$<br>\n",
    "    (b) Repeat step (a) 1000 times to collect 1000 such sample means and plot them using a histogram. <br>\n",
    "    (c) How many of such sample means (out of 1000) are at least 0.1 away from true mean parameter, which is 0.5 for uniform (0, 1)?<br>\n",
    "    (d) Repeat steps (a) to (c) with $n = 100$ and $n = 500$. What conclusion can you make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a48be3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When n = 20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 134 at least 0.1 away from true mean parameterwhen n=20, which is 0.5 for uniform (0, 1)\n",
      "\n",
      "When n = 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 at least 0.1 away from true mean parameterwhen n=100, which is 0.5 for uniform (0, 1)\n",
      "\n",
      "When n = 500\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 at least 0.1 away from true mean parameterwhen n=500, which is 0.5 for uniform (0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [6, 3]\n",
    "\n",
    "def step_a(n):\n",
    "    return np.mean(np.random.uniform(size=n))\n",
    "\n",
    "def step_b(n):\n",
    "    sample_means = []\n",
    "    for i in range(1000):\n",
    "        sample_means.append(step_a(n))\n",
    "    plt.hist(sample_means)\n",
    "    plt.title(f'Sample mean distribution when n={n}')\n",
    "    plt.show()\n",
    "    return sample_means\n",
    "\n",
    "def step_c(sample_means):\n",
    "    return (np.abs(np.array(sample_means)-0.5) > 0.1).sum()\n",
    "\n",
    "# step d\n",
    "ns=[20,100,500]\n",
    "for n in ns:\n",
    "    print(f'When n = {n}')\n",
    "    sample_means = step_b(n)\n",
    "    print(\n",
    "        f'There are {step_c(sample_means)} at least 0.1 away from true mean parameter'\n",
    "        f'when n={n}, which is 0.5 for uniform (0, 1)\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd42c74",
   "metadata": {},
   "source": [
    "As we increase the number of samples, the variance of the distribution of the sample means gets tighter. We can see this in this histograms by looking at how spread out the x-axis is. For n=20, the histogram is most spread and when n=500, the histogram is tightest. We can also see this by calculating how many samples in the distribution are at least 0.1 away from the true mean paramter, which is 0.5. This is largest when n=20 and 0 when n=100 or n=500. This makes sense because as stated above, the sample mean is a consistent estimator, ie its variance becomes 0 as you increase the sample size. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
